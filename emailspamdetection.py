# -*- coding: utf-8 -*-
"""EmailSpamDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yjMwA7nxfdlN8xjjQK9ObLY7ld9Nd9iK

EMAIL Spam Detection using ML
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer

import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import Image, display
# %matplotlib inline

from tqdm.auto import tqdm
import time
from google.colab import files
uploaded=files.upload()

df=pd.read_csv('spam.csv',encoding='latin-1')
df

df.head()

df.tail()

"""PERFORMING EDA

1.Handling Null Values
"""

df.isna().any()

df.isna().sum()

"""2.Handling Duplicate Values"""

df['v2'].nunique()

df.shape

df

"""3.CLASS DISTRIBUTIONS"""

class_counts = df['v1'].value_counts()
class_counts.plot(kind='bar')
plt.title('Class Distribution of Spam/Ham')
plt.xlabel('Spam/Ham')
plt.ylabel('Number of Mails')
plt.show()

"""WORD COUNT"""

from collections import Counter
import re

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

all_text = ' '.join(df['v2'].values)
# Remove URLs, mentions, and hashtags from the text
all_text = re.sub(r'http\S+', '', all_text)
all_text = re.sub(r'@\S+', '', all_text)
all_text = re.sub(r'#\S+', '', all_text)
# Split the text into individual words
words = all_text.split()
# Remove stop words
stop_words = set(stopwords.words('english'))
words = [word for word in words if not word in stop_words]
# Count the frequency of each word
word_counts = Counter(words)
top_words = word_counts.most_common(100)
top_words

top_words = word_counts.most_common(10) # Change the number to show more/less words
x_values = [word[0] for word in top_words]
y_values = [word[1] for word in top_words]
plt.bar(x_values, y_values)
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Most Commonly Used Words')
plt.show()

"""NATUAL LANGUAGE PROCESSING

1. DATA CLEANING
"""

def clean_text(text):
    # Remove HTML tags
    text = re.sub('<.*?>', '', text)
    # Remove non-alphabetic characters and convert to lowercase
    text = re.sub('[^a-zA-Z]', ' ', text).lower()
    # Tokenize the text
    words = nltk.word_tokenize(text)
    # Remove stopwords
    words = [w for w in words if w not in stopwords.words('english')]
    # Stem the words
    stemmer = PorterStemmer()
    words = [stemmer.stem(w) for w in words]
    # Join the words back into a string
    text = ' '.join(words)
    return text

import nltk
nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# tqdm.pandas()
# 
# df['cleaned_text'] = df['v2'].progress_apply(clean_text)

"""2. FEATURE EXTRACTION"""

cv = CountVectorizer(max_features=5000)
X = cv.fit_transform(df['cleaned_text']).toarray()
y = df['v1']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""CLASSIFICATION MODEL

1. Logistic Regression Model
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# train a Logistic Regression Model
clf = LogisticRegression()
clf.fit(X_train, y_train)

"""2. Predictions"""

y_pred = clf.predict(X_test)
y_pred

"""3. ACCURACY"""

acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)

"""4. CONFUSION MATRIX"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

import seaborn as sns
sns.heatmap(cm, annot=True)

cm

"""5. CLASSIFICATION REPORT"""

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(report)